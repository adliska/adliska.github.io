<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Multi-arm bandits and the conflict between exploration and exploitation</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/2016/05/15/reinforcement-learning-sutton-barto-chapter-2-multi-arm-bandits-exploration-exploitation-code.html">

    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-55554430-2', 'auto');
        ga('send', 'pageview');
    </script>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Adam Liska</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
          <a class="page-link" href="/blog">Blog</a>
          <a class="page-link" href="/publications">Publications</a>
          <a class="page-link" href="/notes">Notes</a>
          <a class="page-link" href="/other-projects">Other projects</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Multi-arm bandits and the conflict between exploration and exploitation</h1>
    <p class="post-meta">May 15, 2016</p>
  </header>

  <article class="post-content">
    <p>We have recently started a reading group on <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement 
learning</a> with
a group of colleagues, following the draft of the second edition 
of <a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">Reinforcement Learning: An 
Introduction</a> 
by Sutton and Barto. The PDF version of the book is available on 
the book website; we are using the draft from October 2015 
(currently the latest one).</p>

<p>To better understand the material, I’ve been 
implementing some of the methods and exercises in Python and 
would like to start sharing the code on <a href="https://github.com/adliska/reinforcement_learning">my 
GitHub</a> with some
comments here on the blog, hoping that others may find it useful. 
I’ll try to follow the notation used in the book as much as possible, 
and will assume that you’re familiar with it from the book.</p>

<p>Chapter 2 of the book introduces <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-arm
bandits</a>, 
the <a href="http://www.tomstafford.staff.shef.ac.uk/?p=48">conflict between exploration and 
exploitation</a>, 
and several different methods how to approach and balance this conflict. 
It also introduces a 10-armed testbed on which the methods are evaluated.
The code I wrote for this chapter is available in <a href="https://github.com/adliska/reinforcement_learning/blob/master/Chapter%2002%20Multi-arm%20bandits.ipynb">an IPython 
notebook</a>
and I’m going to describe just parts of it here.</p>

<h2 id="multi-arm-bandits">Multi-arm bandits</h2>
<p>The bandits considered in the chapter have <em>k</em> arms and their action
values (<script type="math/tex">q_*</script>) are drawn from a normal distribution with mean 0 and 
variance 1. When we select a given action (say <script type="math/tex">a</script>), the actual reward 
<script type="math/tex">R_t</script> is drawn from a normal distribution with the action value <script type="math/tex">q_*(a)</script> 
as mean and variance 1.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="c"># k: number of bandit arms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        
        <span class="c"># qstar: action values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qstar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">qstar</span><span class="p">[</span><span class="n">a</span><span class="p">])</span></code></pre></figure>

<p>The goal of the mutli-arm bandit problem is to maximize winnings
by focusing on actions that result in highest rewards. However, since
we (the agent) don’t have access to the underlying action values <script type="math/tex">q_*</script>,
we have to figure out how to strike a balance between exploration, i.e.,
moves in which we try different actions and see what kind of rewards 
they give us, and exploitation, i.e., moves in which we focus on 
actions that we think – given our limited experience – result in 
highest rewards.</p>

<h2 id="action-selection-methods">Action selection methods</h2>

<p>In action value methods that address this problem, the agent keeps 
track of rewards that it 
received after selecting each action and estimates <script type="math/tex">q_*(a)</script> by 
averaging the rewards received after selecting action <script type="math/tex">a</script>. 
This estimate is denoted <script type="math/tex">Q(a)</script>.</p>

<p>In one such method, the <strong><script type="math/tex">\varepsilon</script>-greedy action selection method</strong>, 
the agent
will with probability <script type="math/tex">1 - \varepsilon</script> select the action
that has the highest <script type="math/tex">Q(a)</script> (the <em>greedy</em> action), 
and – with probability <script type="math/tex">\varepsilon</script> – a random action.</p>

<p>This action selection strategy can be coded in Python in the following way:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">epsilon_greedy_action_selection</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">numsteps</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="c"># k: number of bandit arms</span>
    <span class="c"># numsteps: number of steps (repeated action selections)</span>
    <span class="c"># epsilon: probability with which a random action is selected,</span>
    <span class="c">#          as opposed to a greedy action</span>

    <span class="c"># Apossible[t]: list of possible actions at step t</span>
    <span class="n">Apossible</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c"># A[t]: action selected at step t</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numsteps</span><span class="p">,))</span>
    
    <span class="c"># N[a,t]: the number of times action a was selected </span>
    <span class="c">#         in steps 0 through t-1</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">numsteps</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="c"># R[t]: reward at step t</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numsteps</span><span class="p">,))</span>
    
    <span class="c"># Q[a,t]: estimated value of action a at step t</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,(</span><span class="n">numsteps</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>

    <span class="c"># Initialize bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numsteps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="c"># All actions are equally possible</span>
            <span class="n">Apossible</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c"># Select greedy actions as possible actions</span>
            <span class="n">Apossible</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="p">]))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="c"># Select action randomly from possible actions</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">Apossible</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Apossible</span><span class="p">[</span><span class="n">t</span><span class="p">]))]</span>

        <span class="c"># Record action taken</span>
        <span class="n">A</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>

        <span class="c"># Perform action (= sample reward)</span>
        <span class="n">R</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">bandit</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c"># Update action counts</span>
        <span class="n">N</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span>
        <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c"># Update action value estimates, incrementally</span>
        <span class="k">if</span> <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">{</span><span class="s">'bandit'</span> <span class="p">:</span> <span class="n">bandit</span><span class="p">,</span>
            <span class="s">'numsteps'</span> <span class="p">:</span> <span class="n">numsteps</span><span class="p">,</span>
            <span class="s">'epsilon'</span> <span class="p">:</span> <span class="n">epsilon</span><span class="p">,</span>
            <span class="s">'Apossible'</span><span class="p">:</span> <span class="n">Apossible</span><span class="p">,</span> 
            <span class="s">'A'</span><span class="p">:</span> <span class="n">A</span><span class="p">,</span> <span class="s">'N'</span> <span class="p">:</span> <span class="n">N</span><span class="p">,</span> <span class="s">'R'</span> <span class="p">:</span> <span class="n">R</span><span class="p">,</span> <span class="s">'Q'</span> <span class="p">:</span> <span class="n">Q</span><span class="p">}</span></code></pre></figure>

<p>In this implementation, we also keep track of the state of the agent at
every step, and can for example look at how its action value estimates
converge towards the actual action values with the increasing number of steps:</p>

<p><img src="/images/action_value_estimates.png" alt="ahoj" /></p>

<p>The chapter discusses other action selection methods: optimistic initial 
values selection, upper-confidence-bound selection, and gradient bandits.
The code for these methods together with some comparisons between them
are available in the <a href="https://github.com/adliska/reinforcement_learning/blob/master/Chapter%2002%20Multi-arm%20bandits.ipynb">IPython 
notebook</a>
that I already mentioned above.</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

<div class="wrapper">

    <!-- <h2 class="footer-heading">Adam Liska</h2> -->

    <div class="footer-col-wrapper">
        <div class="footer-col  footer-col-1">
            <ul class="contact-list">
                <li><strong>Contact</strong></li>
                <li><a href="mailto:adam.liska@gmail.com">adam.liska@gmail.com</li>
            </ul>
        </div>

        <div class="footer-col  footer-col-2">
            <ul class="social-media-list">
                
                <li>
                <a href="https://github.com/adliska">
                    <span class="icon  icon--github">
                        <svg viewBox="0 0 16 16">
                        <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                        </svg>
                    </span>

                    <span class="username">adliska</span>
                </a>
                </li>
                

                
                <li>
                <a href="https://twitter.com/adliska">
                    <span class="icon  icon--twitter">
                        <svg viewBox="0 0 16 16">
                        <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                        c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                        </svg>
                    </span>

                    <span class="username">adliska</span>
                </a>
                </li>
                
                <li>
                <a class="page-link" href="/feed.xml">
                    <span class="icon">
                        <svg version="1.1" id="Icons" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 100 100" style="enable-background:new 0 0 100 100;" xml:space="preserve">
                        <path id="RSS" d="M25.4,74.569c2.18,2.177,3.506,5.131,3.514,8.42c-0.008,3.283-1.334,6.237-3.506,8.406L25.4,91.399
                        c-2.176,2.166-5.148,3.49-8.453,3.49c-3.301,0-6.266-1.328-8.441-3.494C6.336,89.227,5,86.276,5,82.989
                        c0-3.289,1.336-6.25,3.506-8.423v0.003c2.176-2.168,5.141-3.502,8.441-3.502C20.252,71.067,23.221,72.404,25.4,74.569z M25.4,74.566
                        v0.003c0.004,0,0.004,0,0.004,0L25.4,74.566z M8.506,91.399c0,0,0,0,0-0.004c-0.004,0-0.004,0-0.004,0L8.506,91.399z M5.01,35.604
                        v17.185c11.219,0.007,21.83,4.414,29.771,12.367c7.939,7.928,12.328,18.555,12.346,29.813h0.006V95h17.256
                        c-0.014-16.37-6.668-31.198-17.426-41.964C36.203,42.272,21.381,35.614,5.01,35.604z M5.041,5v17.177
                        C45.145,22.206,77.746,54.858,77.773,95H95c-0.012-24.81-10.096-47.288-26.393-63.596C52.305,15.101,29.842,5.011,5.041,5z"/>
                        </svg>
                    </span>

                    <span class="username">RSS</span>
                </a>
                </li>
            </ul>
        </div>

        <div class="footer-col  footer-col-3">
            <p class="text"></p>
        </div>
    </div>

</div>

</footer>


  </body>

</html>
